# Common settings across all pipeline types
common:
  # Logging configuration
  logging:
    level: INFO
    format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
    file_level: DEBUG
    console_level: INFO
    log_dir: "logs"

  custom_ml_model:
    enabled: true  # Toggle for custom model flow
    function_path: "/home/devendra_yadav/custom_ml_model/data/custom_code/123456789/v1/custom_code.py"  # Path to Python module with custom function
    function_name: "run_custom_pipeline"  # Name of function to call
  
  # Output settings
  output:
    base_dir: "outputs"
    save_metadata: true
    max_versions: 5
  
  # Data processing
  preprocessing:
    # Outlier handling
    outlier_detection:
      method: "iqr"  # Options: "iqr", "zscore"
      threshold: 1.5  # For IQR: typically 1.5, For zscore: typically 3.0
      strategy: "clip"  # Options: "clip", "remove", "none"
    
    # Missing value handling
    missing_values:
      numeric_strategy: "mean"  # Options: "mean", "median", "most_frequent", "constant"
      categorical_strategy: "most_frequent"  # Options: "most_frequent", "constant"
  
  # Feature engineering
  feature_engineering:
    scaling: "standard"  # Options: "standard", "minmax", "robust", "none"
  
  # Train/Test split
  train_test_split:
    test_size: 0.2
    random_state: 42
    stratify: true  # Will be ignored for regression

# Regression specific settings
regression:
  models:
    enabled:
      - linear_regression
      - ridge
      - lasso
      - elastic_net
      - decision_tree
      - random_forest
      - gradient_boosting
    
    # Model specific parameters
    parameters:
      # Linear models
      ridge:
        alpha: 1.0
      lasso:
        alpha: 0.1
      elastic_net:
        alpha: 0.1
        l1_ratio: 0.5
      
      # Tree-based models
      decision_tree:
        max_depth: 10
      random_forest:
        n_estimators: 100
        max_depth: null
      gradient_boosting:
        n_estimators: 100
        learning_rate: 0.1
      
      
  random_search:
    enabled: true
    n_iter: 20      # Number of parameter combinations to try
    cv: 5           # Cross-validation folds
    verbose: 1
    models:
      random_forest:
        enabled: true
        parameters:
          n_estimators: [50, 100, 150, 200]
          max_depth: [5, 10, 15, 20, 25, 30, null]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 4]
          max_features: ["sqrt", "log2", null]
      
      gradient_boosting:
        enabled: true
        parameters:
          n_estimators: [50, 100, 150, 200]
          learning_rate: [0.01, 0.05, 0.1, 0.15, 0.2]
          max_depth: [2, 3, 4, 5, 6]
          min_samples_split: [2, 5, 10]
          subsample: [0.7, 0.8, 0.9, 1.0]
      
      elastic_net:
        enabled: true
        parameters:
          alpha: [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0, 10.0]
          l1_ratio: [0.1, 0.3, 0.5, 0.7, 0.9]
      
      ridge:
        enabled: false
        parameters:
          alpha: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
  

  
  # Evaluation metrics
  evaluation:
    primary_metric: "r2"  # The metric used to select the best model
    metrics:
      - r2
      - rmse
      - mae
      - mape

    # Prediction storage settings
    store_predictions: true  # Whether to store test set predictions
    max_stored_predictions: 1000  # Maximum number of predictions to include in metadata

# Classification specific settings
classification:  
  class_handling:
    balance_method: null  # Options: null, "smote", "class_weight"
    multi_class: "auto"  # Options: "auto", "ovr", "multinomial"
  
  # Standard models configuration
  models:
    # Which models to train if custom models not used
    enabled:
      - logistic_regression
      - decision_tree
      - random_forest
      - gradient_boosting
      - knn
      - naive_bayes
    
    # Model specific parameters
    parameters:
      logistic_regression:
        C: 1.0
        max_iter: 1000
      decision_tree:
        max_depth: 10
      random_forest:
        n_estimators: 100
        max_depth: null
      gradient_boosting:
        n_estimators: 100
        learning_rate: 0.1
      knn:
        n_neighbors: 5
  
  # Random Search configuration
  random_search:
    enabled: true
    n_iter: 20      # Number of parameter combinations to try
    cv: 5           # Cross-validation folds
    verbose: 1
    models:
      random_forest:
        enabled: true
        parameters:
          n_estimators: [50, 100, 150, 200]
          max_depth: [5, 10, 15, 20, 25, null]
          min_samples_split: [2, 5, 10]
          min_samples_leaf: [1, 2, 4]
          max_features: ["sqrt", "log2", null]
      
      gradient_boosting:
        enabled: true
        parameters:
          n_estimators: [50, 100, 150, 200]
          learning_rate: [0.01, 0.05, 0.1, 0.2]
          max_depth: [2, 3, 4, 5]
          min_samples_split: [2, 5, 10]
          subsample: [0.7, 0.8, 0.9, 1.0]
      
      logistic_regression:
        enabled: true
        parameters:
          C: [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
          solver: ["lbfgs", "liblinear", "saga"]
          penalty: ["l1", "l2", "elasticnet"]
          max_iter: [1000]
      
      knn:
        enabled: true
        parameters:
          n_neighbors: [3, 5, 7, 9, 11, 13]
          weights: ["uniform", "distance"]
          p: [1, 2]  # Manhattan or Euclidean distance
  
  # Evaluation metrics
  evaluation:
    primary_metric: "accuracy"  # The metric used to select the best model
    metrics:
      - accuracy
      - precision
      - recall
      - f1
    # Prediction storage settings
    store_predictions: true  # Whether to store test set predictions
    max_stored_predictions: 1000  # Maximum number of predictions to include in metadata

# Clustering specific settings
clustering:
  
  # Preprocessing
  preprocessing:
    # Feature selection
    feature_selection: 
      enabled: false
    # Scaling method
    scaling: "standard"  # Options: "standard", "minmax", "robust", "none"
  
  # Dimensionality reduction
  dimensionality_reduction:
    method: "pca"  # Options: "pca", "tsne", "none"
    n_components: 2  # Number of components to reduce to
    
  # Models configuration
  models:
    # Which models to train
    enabled:
      - kmeans
      - agglomerative
      - gmm
      - dbscan
      - birch
    
    # Default parameters
    parameters:
      kmeans:
        n_init: 10
      dbscan:
        eps_auto: true  # Auto-estimate epsilon
      
  # Evaluation metrics
  evaluation:
    primary_metric: "silhouette"  # The metric used to select the best model
    metrics:
      - silhouette
      - calinski_harabasz
      - davies_bouldin

# Time series specific settings
time_series:
  # Preprocessing
  preprocessing:
    # Feature extraction
    feature_extraction:
      lag_orders: [1, 7, 14, 30]
      rolling_windows: [7, 14, 30]
      diff_orders: [1, 7]
    # Scaling method
    scaling: "minmax"  # Options: "standard", "minmax", "robust", "none"
  
  # Models configuration
  models:
    # Which models to train
    enabled:
      - RandomForest
      - GradientBoosting
      - LinearRegression
      - Ridge
      - ARIMA
    
    # ARIMA parameters
    arima:
      order: [1, 1, 1]
    
    # Evaluation metrics
    evaluation:
      primary_metric: "rmse"  # The metric used to select the best model